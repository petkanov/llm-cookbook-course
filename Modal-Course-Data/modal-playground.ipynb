{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Modal Platform?\n",
    "Modal is a cloud platform that simplifies deploying and scaling machine learning inferencing workloads. \n",
    "\n",
    "Modal lets you focus on your models while it handles infrastructure, scaling, and cost optimization automatically.\n",
    "### Setting Up Modal\n",
    "Follow these steps to get started with Modal:\n",
    "#### 1. Create an Account\n",
    "- Visit [modal.com](https://modal.com) and sign up for a new account.\n",
    "#### 2. Install the Modal Python Package\n",
    "```bash\n",
    "pip install modal\n",
    "```\n",
    "#### 3. Authenticate with Modal\n",
    "```bash\n",
    "modal setup\n",
    "```\n",
    "or\n",
    "```bash\n",
    "python -m modal setup\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modal\n",
    "\n",
    "app = modal.App(\"My-App\")\n",
    "\n",
    "@app.function()\n",
    "def sum(x: int, y: int) -> None:\n",
    "    print( x + y )\n",
    "\n",
    "@app.function()\n",
    "def square(x: int) -> None:\n",
    "    print( x ** 2 )\n",
    "\n",
    "\n",
    "@app.local_entrypoint()\n",
    "def main(x: int) -> None:\n",
    "    square.local(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrypoints for Ephemeral Apps\n",
    "The code that runs first when you do ```modal run``` is called the \"entrypoint\"\n",
    "### Argument parsing\n",
    "```bash\n",
    "    modal run script.py --x 4\n",
    "```\n",
    "To run a specific function locally\n",
    "```bash\n",
    "    modal run script.py::sum --x 5 --y 8\n",
    "```\n",
    "### Deploy a function\n",
    "```bash\n",
    "    modal deploy script.py\n",
    "```\n",
    "Then in terminal or another python App\n",
    "```bash\n",
    "    python\n",
    "    >>> import modal\n",
    "    >>> sum_function = modal.Function.from_name(\"My-App\", \"sum\")\n",
    "    >>> sum_function.remote(6, 7)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Infrastructure then Running it locally and in the cloud "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modal\n",
    "from modal import App, Image\n",
    "\n",
    "app = App(\"Location-Function\")\n",
    "\n",
    "@app.function(image = Image.debian_slim().pip_install(\"requests\"))\n",
    "def my_location():\n",
    "    import requests\n",
    "\n",
    "    location_data = requests.get('http://ip-api.com/json').json()\n",
    "\n",
    "    city, country, ip_address = location_data['city'], location_data['country'], location_data['query']\n",
    "    temperature = requests.get(f\"https://wttr.in/{city}?format=%t&m\").text.strip()\n",
    "\n",
    "    response = f\"Code running on IP {ip_address} ({city}, {country}) Outside is: {temperature}\"\n",
    "    print(response)\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "with modal.enable_output():\n",
    "    with app.run():\n",
    "        my_location.local()\n",
    "        # my_location.remote()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API endpoints\n",
    "To turn this function into a web endpoint run: \n",
    "```bash \n",
    "    modal serve web_api_function.py\n",
    "```\n",
    "In the output, you should see a URL that ends with ```hello-dev.modal.run```\n",
    "\n",
    "If you add ```/docs``` to the end of the URL you can also find interactive documentation, powered by OpenAPI and Swagger\n",
    "\n",
    "By running the endpoint with ```modal serve```, you created a temporary endpoint that will disappear if you interrupt your terminal\n",
    "\n",
    "To deploy this endpoint permanently, run \n",
    "```bash\n",
    "    modal deploy web_api_function.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modal\n",
    "from modal import App, Image\n",
    "\n",
    "app = App(image = Image.debian_slim().pip_install(\"fastapi[standard]\"))\n",
    "\n",
    "@app.function()\n",
    "@modal.web_endpoint(docs=True)\n",
    "def greet(user: str) -> str:\n",
    "    return f\"Hello {user}!\"\n",
    "\n",
    "@app.function()\n",
    "@modal.web_endpoint(method=\"POST\", docs=True)\n",
    "def square(item: dict):\n",
    "    return {\"value\": item['x']**2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy in a Class to Handle expensive startup (heavy model loading)\n",
    "\n",
    "```@modal.enter()``` lifecycle hook happens after container started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modal\n",
    "from modal import App, Image\n",
    "\n",
    "app = App(\"web-app\", image = Image.debian_slim().pip_install(\"fastapi[standard]\") )\n",
    "\n",
    "@app.cls(cpu=1, memory=\"1Gi\")\n",
    "class WebApp:\n",
    "    @modal.enter()\n",
    "    def startup(self):\n",
    "        from datetime import datetime, timezone\n",
    "\n",
    "        print(\"Container started -> Start up time initiated!\")\n",
    "        self.start_time = datetime.now(timezone.utc)\n",
    "\n",
    "    @modal.method()\n",
    "    def ping(self):\n",
    "        return \"pong\"\n",
    "\n",
    "    @modal.web_endpoint(docs=True)\n",
    "    def web(self):\n",
    "        from datetime import datetime, timezone\n",
    "\n",
    "        current_time = datetime.now(timezone.utc)\n",
    "        return {\"start_time\": self.start_time, \"current_time\": current_time}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping it alive\n",
    "Ater some time, if API is not used, Modal will kill the containers along with our application\n",
    "\n",
    "We need to create a periodinc cron job that will be pinging it to refresh it's state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modal\n",
    "\n",
    "WebApp = modal.Cls.from_name(\"web-app\", \"WebApp\")\n",
    "web_app = WebApp()\n",
    "\n",
    "reply = web_app.ping.remote()\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying a HuggingFace model in a Modal Class\n",
    "Possible Auth Error:\n",
    "```\n",
    "Cannot access gated repo for url https://huggingface.co/google/gemma-2-2b-it/resolve/main/config.json.\n",
    "Access to model google/gemma-2-2b-it is restricted and you are not in the authorized list. Visit https://huggingface.co/google/gemma-2-2b-it to ask for access.\n",
    "```\n",
    "Follow suggested link and click on ```Accept terms / Grant access``` or similar promts that you will see on a model page\n",
    "\n",
    "Run\n",
    "```bash\n",
    "    modal deploy model_class.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modal\n",
    "from modal import App, Image\n",
    "\n",
    "app = App(\"gemma-webapp\")\n",
    "image = (\n",
    "    Image.debian_slim()\n",
    "    .apt_install(\"git\")\n",
    "    .pip_install(\"torch\", \"transformers\", \"huggingface_hub\", \"fastapi[standard]\", \"accelerate\")\n",
    "    .run_commands(\"git config --global credential.helper store\")\n",
    ")\n",
    "secrets = [modal.Secret.from_name(\"hf-token\")]\n",
    "\n",
    "@app.cls(image=image, secrets=secrets, gpu=\"T4\", container_idle_timeout=1200)\n",
    "class GemmaModelApp:\n",
    "    @modal.enter()\n",
    "    def startup(self):\n",
    "        import os, torch\n",
    "        from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "        from huggingface_hub import login\n",
    "\n",
    "        hf_token = os.environ['HF_TOKEN']\n",
    "        login(hf_token, add_to_git_credential=True)\n",
    "\n",
    "        print(\"Loading tokenizer...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "        print(\"Loading model...\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b-it\", device_map=\"auto\")\n",
    "\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.tokenizer.padding_side = \"right\" \n",
    "\n",
    "        print(\"Model loaded successfully!\")\n",
    "\n",
    "    @modal.method()\n",
    "    def ping(self):\n",
    "        from datetime import datetime, timezone\n",
    "        return f\"pong@{datetime.now(timezone.utc)}\"\n",
    "        \n",
    "    @modal.method()\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        print(f\"Received prompt: {prompt}\")\n",
    "        return self._generate_response(prompt)\n",
    "\n",
    "    @modal.web_endpoint(method=\"POST\", docs=True)\n",
    "    def web_generate(self, prompt: str) -> str:\n",
    "        print(f\"Web Controller Received prompt: {prompt}\")\n",
    "        return self._generate_response(prompt)\n",
    "    \n",
    "    def _generate_response(self, prompt: str) -> str:\n",
    "        import torch\n",
    "        \n",
    "        inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        attention_mask = torch.ones(inputs.shape, device=\"cuda\")\n",
    "        outputs = self.model.generate(\n",
    "            inputs,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=500,     # Generate up to 100 tokens\n",
    "            num_return_sequences=1, # Ensures that the model generates only one response. Increase this number if you want the model to generate multiple variations of the response.\n",
    "            do_sample=True,         # Enables sampling of (temperature, top_k, top_p) parameters for more diverse responses\n",
    "            temperature=0.7,        # Controls randomness (lower is more deterministic)\n",
    "            top_k=50,               # Only considers the top 50 tokens with the highest probabilities\n",
    "            top_p=0.9,              # Implements nucleus sampling for more diverse and natural responses.\n",
    "        )\n",
    "\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and for the state refreshing cron job:\n",
    "```bash\n",
    "    python cron_job.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import modal\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "GemmaModelApp = modal.Cls.from_name(\"gemma-webapp\", \"GemmaModelApp\")\n",
    "gemma_service = GemmaModelApp()\n",
    "\n",
    "while True:\n",
    "    reply = gemma_service.ping.remote()\n",
    "    print(f\"ping@{datetime.now(timezone.utc)}: {reply}\")\n",
    "    time.sleep(600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Volumes\n",
    "Modal Volumes allow you to efficiently save and access large files, such as machine learning model weights, across different applications and sessions. \n",
    "\n",
    "This capability ensures that your trained models are readily available for fast deployment and inference without the need for repeated uploads or processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving models weight to a Volume, then reading it from there and initializing weight for the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from modal import App, Volume, Image\n",
    "\n",
    "app = App(\"Volumes-test\")\n",
    "\n",
    "MODEL_DIR = Path(\"/models\")\n",
    "\n",
    "image = (\n",
    "    Image.debian_slim()\n",
    "    .pip_install(\"huggingface_hub[hf_transfer]\",\"transformers\",\"torch\", \"accelerate\")  \n",
    "    .env({\"HF_HUB_ENABLE_HF_TRANSFER\": \"1\"}) \n",
    ")\n",
    "volume = Volume.from_name(\"testing-model-weights-directory\", create_if_missing=True)\n",
    "\n",
    "@app.function(\n",
    "    volumes={MODEL_DIR: volume},  # \"mount\" the Volume, sharing it with your function\n",
    "    image=image,  \n",
    ")\n",
    "def download_model(\n",
    "    repo_id: str=\"facebook/opt-125m\", # small model for testing purposes\n",
    "    revision: str=None,  # include specific revision (commit hash)\n",
    "    ):\n",
    "    from huggingface_hub import snapshot_download\n",
    "\n",
    "    model_path = MODEL_DIR / repo_id\n",
    "\n",
    "    if not model_path.exists() or not any(model_path.glob(\"*\")):  # directory exists and contains some content (files)\n",
    "        print(f\"Model not found at {model_path}, downloading...\")\n",
    "\n",
    "        snapshot_download(repo_id=repo_id, local_dir=model_path, revision=revision)\n",
    "\n",
    "        print(f\"Model downloaded to {model_path}\")\n",
    "    else:\n",
    "        print(f\"Model already exists at {model_path}\")\n",
    "\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    import torch\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "    prompt = \"Hello, how are you?\"\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    attention_mask = torch.ones(inputs.shape)\n",
    "\n",
    "    output = model.generate(inputs, attention_mask=attention_mask, max_new_tokens=55, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(\"Generated Text:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a vLLM Engine and Deploy It\n",
    "\n",
    "vLLM is an advanced, high-performance serving framework designed for large language models (LLMs). It optimizes model execution by leveraging a cutting-edge, tensor-parallel engine and dynamic memory management to efficiently handle complex queries. By supporting models such as OpenAI's GPT and LLaMA, vLLM offers faster response times and better scalability, making it ideal for real-time LLM applications. It integrates seamlessly with popular machine-learning frameworks and emphasizes low latency and high throughput for serving language models in production environments.\n",
    "\n",
    "\n",
    "The vLLM server, which is compatible with OpenAI, is presented as a FastAPI router. \n",
    "\n",
    "The process begins by creating an `AsyncLLMEngine`, which is the central component of the vLLM server. \n",
    "\n",
    "This engine handles model loading, executes inference operations, and facilitates response delivery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Download Service\n",
    "Run\n",
    "```bash\n",
    "    modal deploy download-model-service.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modal import App, Volume, Image, Secret\n",
    "from pathlib import Path\n",
    "\n",
    "MODELS_DIR = \"/llm\"\n",
    "DEFAULT_NAME = \"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16\"\n",
    "DEFAULT_REVISION = \"a7c09948d9a632c2c840722f519672cd94af885d\"\n",
    "MINUTES = 60\n",
    "HOURS = 60 * MINUTES\n",
    "\n",
    "volume = Volume.from_name(\"llm\", create_if_missing=True)\n",
    "image = (\n",
    "    Image.debian_slim(python_version=\"3.10\")\n",
    "    .pip_install([\"huggingface_hub\", \"hf-transfer\"])\n",
    "    .env({\"HF_HUB_ENABLE_HF_TRANSFER\": \"1\"}) # enable the faster download method\n",
    ")\n",
    "\n",
    "app = App(\"download-model-service\",\n",
    "    image=image,\n",
    "    secrets=[Secret.from_name(\"hf-token\", required_keys=[\"HF_TOKEN\"])]\n",
    ")\n",
    "\n",
    "@app.function(\n",
    "        volumes={MODELS_DIR: volume}, \n",
    "        timeout=4 * HOURS ) # how long the function will run before being terminated\n",
    "def download_model(model_name: str = DEFAULT_NAME, \n",
    "                   model_revision: str = DEFAULT_REVISION, \n",
    "                   force_download: bool = False):\n",
    "    from huggingface_hub import snapshot_download\n",
    "\n",
    "    volume.reload() # Reload the volume to ensure it is available\n",
    "\n",
    "    snapshot_download(\n",
    "        model_name,\n",
    "        local_dir=MODELS_DIR + \"/\" + model_name,\n",
    "        ignore_patterns=[\n",
    "            \"*.pt\",\n",
    "            \"*.bin\",\n",
    "            \"*.pth\",\n",
    "            \"original/*\",\n",
    "        ],\n",
    "        revision=model_revision,\n",
    "        force_download=force_download,\n",
    "    )\n",
    "    \n",
    "    completion_file = Path(MODELS_DIR) / model_name / \"download_complete\"\n",
    "    completion_file.touch()  # Create an empty file to signal completion\n",
    "\n",
    "    volume.commit() # Commit changes to volume to ensure visibility to other functions\n",
    "    \n",
    "\n",
    "@app.local_entrypoint()\n",
    "def main(model_name: str = DEFAULT_NAME, model_revision: str = DEFAULT_REVISION, force_download: bool = False):\n",
    "    download_model.remote(model_name, model_revision, force_download)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferencing Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modal\n",
    "from pathlib import Path\n",
    "\n",
    "vllm_image = modal.Image.debian_slim(python_version=\"3.12\").pip_install(\n",
    "    \"vllm==0.6.3post1\", \"fastapi[standard]==0.115.4\"\n",
    ")\n",
    "\n",
    "MODELS_DIR = \"/llm\"\n",
    "MODEL_REPO_ID = \"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16\" # quantized to 4-bit\n",
    "MODEL_REVISION = \"a7c09948d9a632c2c840722f519672cd94af885d\"\n",
    "\n",
    "try:\n",
    "    volume = modal.Volume.lookup(\"llm\", create_if_missing=False)\n",
    "except modal.exception.NotFoundError:\n",
    "    raise RuntimeError(\"Volume not found. Please deploy [download-model-service] first\")\n",
    "\n",
    "app = modal.App(\"vllm-chat\")\n",
    "\n",
    "N_GPU = 1  # first upgrade to more powerful GPUs, and only then increase GPU count\n",
    "TOKEN = \"my-token\"  # auth token, for production use, replace with a modal.Secret\n",
    "\n",
    "MINUTE = 60\n",
    "HOUR = 60 * MINUTE\n",
    "\n",
    "\n",
    "@app.function(\n",
    "    image=vllm_image,\n",
    "    gpu=modal.gpu.T4(count=N_GPU),\n",
    "    container_idle_timeout=5 * MINUTE, # how long the container will wait for new requests before shutting down\n",
    "    volumes={MODELS_DIR: volume},\n",
    "    timeout=24 * HOUR, # how long the function will run before being terminated\n",
    "    allow_concurrent_inputs=1000 # how many requests can be processed concurrently\n",
    ")\n",
    "@modal.asgi_app()\n",
    "def serve():\n",
    "    import fastapi, time\n",
    "    from pathlib import Path\n",
    "    import vllm.entrypoints.openai.api_server as api_server\n",
    "    from vllm.engine.arg_utils import AsyncEngineArgs\n",
    "    from vllm.engine.async_llm_engine import AsyncLLMEngine\n",
    "    from vllm.entrypoints.logger import RequestLogger\n",
    "    from vllm.entrypoints.openai.serving_chat import OpenAIServingChat\n",
    "    from vllm.entrypoints.openai.serving_completion import OpenAIServingCompletion\n",
    "    from vllm.entrypoints.openai.serving_engine import BaseModelPath\n",
    "    from vllm.usage.usage_lib import UsageContext\n",
    "\n",
    "    volume.reload() \n",
    "\n",
    "    model_path = Path(MODELS_DIR) / MODEL_REPO_ID\n",
    "    if not model_path.exists() or not any(model_path.glob(\"*\")): \n",
    "        print(f\"Model not found at {model_path}, downloading...\")\n",
    "\n",
    "        download_model_function = modal.Function.from_name(\"download-model-service\", \"download_model\")\n",
    "        download_model_function.remote(MODEL_REPO_ID, MODEL_REVISION)\n",
    "\n",
    "        while not is_model_downloaded(model_path):\n",
    "            print(f\"Model not downloaded yet, waiting...\")\n",
    "            time.sleep(5)\n",
    "            volume.reload() \n",
    "\n",
    "        print(f\"Model downloaded to {model_path}\")\n",
    "    else:\n",
    "        print(f\"Model already exists at {model_path}\")\n",
    "\n",
    "\n",
    "    web_app = fastapi.FastAPI(\n",
    "        title=f\"OpenAI-compatible {MODEL_REPO_ID} server\",\n",
    "        description=\"Run an OpenAI-compatible LLM server with vLLM on modal.com 🚀\",\n",
    "        version=\"0.0.1\",\n",
    "        docs_url=\"/docs\",\n",
    "    )\n",
    "\n",
    "    http_bearer = fastapi.security.HTTPBearer(\n",
    "        scheme_name=\"Bearer Token\",\n",
    "        description=\"See code for authentication details.\",\n",
    "    )\n",
    "    web_app.add_middleware(\n",
    "        fastapi.middleware.cors.CORSMiddleware,\n",
    "        allow_origins=[\"*\"],\n",
    "        allow_credentials=True,\n",
    "        allow_methods=[\"*\"],\n",
    "        allow_headers=[\"*\"],\n",
    "    )\n",
    "\n",
    "    async def is_authenticated(api_key: str = fastapi.Security(http_bearer)):\n",
    "        if api_key.credentials != TOKEN:\n",
    "            raise fastapi.HTTPException(\n",
    "                status_code=fastapi.status.HTTP_401_UNAUTHORIZED,\n",
    "                detail=\"Invalid authentication credentials\",\n",
    "            )\n",
    "        return {\"username\": \"authenticated_user\"}\n",
    "\n",
    "    router = fastapi.APIRouter(dependencies=[fastapi.Depends(is_authenticated)])\n",
    "\n",
    "    router.include_router(api_server.router)\n",
    "    web_app.include_router(router)\n",
    "\n",
    "    engine_args = AsyncEngineArgs(\n",
    "        model=MODELS_DIR + \"/\" + MODEL_REPO_ID,\n",
    "        tensor_parallel_size=N_GPU, # allows model computations to be distributed across multiple GPUs for better performance\n",
    "        gpu_memory_utilization=0.90, #  allowing the engine to use up to 90% of available GPU memory to balance between performance and resource allocation\n",
    "        max_model_len=8096, # maximum number of tokens the model can process in a single request\n",
    "        enforce_eager=False, \n",
    "    )\n",
    "\n",
    "    engine = AsyncLLMEngine.from_engine_args(\n",
    "        engine_args, usage_context=UsageContext.OPENAI_API_SERVER\n",
    "    )\n",
    "\n",
    "    model_config = get_model_config(engine)\n",
    "\n",
    "    request_logger = RequestLogger(max_log_len=2048)\n",
    "\n",
    "    base_model_paths = [\n",
    "        BaseModelPath(name=MODEL_REPO_ID.split(\"/\")[1], model_path=MODEL_REPO_ID)\n",
    "    ]\n",
    "\n",
    "    api_server.chat = lambda s: OpenAIServingChat(\n",
    "        engine,\n",
    "        model_config=model_config,\n",
    "        base_model_paths=base_model_paths,\n",
    "        chat_template=None,\n",
    "        response_role=\"assistant\",\n",
    "        lora_modules=[],\n",
    "        prompt_adapters=[],\n",
    "        request_logger=request_logger,\n",
    "    )\n",
    "    api_server.completion = lambda s: OpenAIServingCompletion(\n",
    "        engine,\n",
    "        model_config=model_config,\n",
    "        base_model_paths=base_model_paths,\n",
    "        lora_modules=[],\n",
    "        prompt_adapters=[],\n",
    "        request_logger=request_logger,\n",
    "    )\n",
    "\n",
    "    return web_app\n",
    "\n",
    "# The function call aims to obtain the model's configuration from the engine. \n",
    "# This configuration may include parameters like the model's architecture, \n",
    "# input/output size, supported features, or other meta-information about the model.\n",
    "def get_model_config(engine):\n",
    "    import asyncio\n",
    "\n",
    "    try:\n",
    "        event_loop = asyncio.get_running_loop()\n",
    "    except RuntimeError:\n",
    "        event_loop = None\n",
    "\n",
    "    if event_loop is not None and event_loop.is_running():\n",
    "        model_config = event_loop.run_until_complete(engine.get_model_config())\n",
    "    else:\n",
    "        model_config = asyncio.run(engine.get_model_config())\n",
    "\n",
    "    return model_config\n",
    "\n",
    "def is_model_downloaded(model_path: Path) -> bool:\n",
    "    completion_file = model_path / \"download_complete\"\n",
    "    return completion_file.exists()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
