[Intro Slide: Data Collection and Preparation – Where it All Begins]

Hey everyone, welcome to this section where we will start unpacking what's really going on inside the so-called term - "artificial intelligence",
and especially - large language models, that most of us by now, can not imagine working or live without.

[Visual: Quick montage of news headlines, social media posts, and video clips all mentioning "AI". Overlay: the letters "A.I." appear everywhere on screen.]

This term we hear and see on every single article, on every video, every news, like everything and  
everywhere nowadays is spammed with these two letters "A.I.". So much hype, it's just crazy.
There are even people for which this even looks like some sort of a mysterious intelligent entity that is living somewhere in a box! 
But once we drill down into the reality behind the "AI" label and strip away all the marketing hype 
- you will see there is absolutely nothing intelligent about it. 
These models in their original base form are not reasoning, they are not having insights or understanding the world like we do. 
In its essence under the hood - It's just a sophisticated statistical calculator that is spitting off the next most likely word to follow.

[Visual: Animation of a black box labeled "AI". The box opens to reveal gears, numbers, and a calculator inside.]

So what's really happening and where does it all begin?
Inside the LLM, it's all about predicting the next word — well, actually, not even the whole word. 
Most of the time, it's just a fragment of a word, that is called a token. 
And each token corresponds to a number. And the model's job is to spit out one number at a time 
using statistical mathematics, based on everything it has already predicted prior to that word.
So the main task of LLM is to predict the next token, based on all previously predicted tokens.

[Visual: Close-up animation of a sentence being typed, but each word appears as a sequence of colored blocks (tokens). Each block flashes as it is "predicted".]

[Slide: "AI" = statistical prediction, not human thinking]

There's absolutely no actual thinking happening — just an advanced pattern matching and probability calculations.
So by the end of these lessons, you'll see for yourself exactly how these systems work.
You will understand not only how this imitation of an intelligent output is generated, 
but also how you can use these tools in your own projects, so that you can build applications that are smart, 
extremely powerful and can make a real difference in the world.

[Visual: Split screen. Left: Human brain with lightbulbs and ideas. Right: Computer with numbers and graphs. Text: "Pattern matching, not thinking"]

Now let's start our AI exploration journey and together, we are going to debunk 
the myth of intelligence in this "AI" Marketing term — and, more importantly,
we will learn how to use these tools' true power for our own benefit.

[Natural pause. Shift to Section 1.]

[Visual: Map or journey graphic, with a path labeled "AI Exploration" and milestones for each lesson.]

So let's begin with the very first step: 
We need somehow to teach this computer program, or a model, to speak a new language — and not just perfectly, 
but as richly and creatively as possible. How how would you do that?
You wouldn't just throw them a single book, or make them listen to the same conversation over and over.
Instead, you'd expose them to all kinds of books, news articles, blogs, scripts, casual chats — 
pretty much every piece of text that you could get your hands on.
The more diverse, comprehensive, and high-quality the text content is, the better our program's eventual language skills will be.

[Visual: Person (cartoon or real) surrounded by a swirling cloud of books, newspapers, web pages, chat bubbles, and scripts.]

So now we got our first problem - how do we actually collect and prepare that massive amount of text that our model will learn from?

[Visual: Giant funnel graphic. At the top: icons for books, websites, forums, chats. At the bottom: a hard drive labeled "Training Data".]

And we are talking about collecting data in the scale of tens of terabytes, that contains hundreds of billions of words.
The most famous and available example of something that we are looking for - is the FineWeb dataset, that's managed by Hugging Face.
It's exactly what we need - a filtered, high quality set of internet text that weighs in at around 44 terabytes.
which by today's measures it's actually really not that much.
But how did they do it and what is it and where does all this data even come from?

[Slide: FineWeb by Hugging Face – 44TB of filtered text]
[Visual: Screenshot of the FineWeb dataset page on Hugging Face. Overlay: "44TB" in bold.]

So most of it originates from what's called the Common Crawl.
Think of Common Crawl as the giant web archive of our time.
Since I think 2007, this Common Crawl has been systematically reading and archiving the public internet.
And by now, it has crawled nearly 3 billion web pages.
But this isn't just about website addresses — no, - it's their entire content: which is articles, documentation, forums, chats and more.

[Visual: Animation of a spider or robot crawling across a globe, collecting web pages into a giant digital library.]

But of course, not every piece of internet text is useful, or even worth feeding into our model.
Raw web data is really messy. Think of this - it includes spam, duplicated copy-paste pages, messed up low-quality content, 
and of course - texts in many, many different languages. 
So, now we need to clean and filter all this data carefully.

[Visual: Messy pile of documents, some with red Xs (spam, duplicates, foreign languages), others with green checks (good content).]

[Slide: Keyword – Filtering: Making Raw Data Usable]

And we arrived at our first step, which is - URL blocklists.

Think of it as a gitignore file in which you would write all the files or directories that you do not want to include in your commits.
But in Common Crawl case it's a file that includes website addresses, the content of which it will exclude from crawling and scraping.
And why do we want to do this? 
That's because all those websites from that list are known for malware, spam, adult content, or whatever else unwanted material.

[Visual: List of URLs on screen, some highlighted in red and crossed out. Overlay: "BLOCKED" stamp.]

Ok, so we managed to avoid a few forbidden websites, we scraped the others and now we got all that websites data.
So what's next? 
Next we need to extract the actual text from HTML markup.
And as you know, the Web content is wrapped up in a lot of code — HTML, JavaScript, CSS.
What we need is to strip all that away, so that we get the text that people actually read.

[Screen demo: Split screen. Left: Raw HTML code. Right: Clean, readable text. Animation: HTML tags fade away, leaving just the text.]

Then comes language filtering.
Typically, we want the majority of our dataset to be in English, since that is where our model is expected to be the most fluent in.
A common threshold is to require that at least 65% of a website content should be in English in order for its content to be included.

[Slide: Language filtering – Pie chart: >65% English]
[Visual: Pie chart animation showing a slice labeled "English" growing to 65% or more.]

Okay, now we got a text, we filtered away all the unnecessary code and data. 
And now we ended up with a nice and clean English text only. 
But we're still not finished. No Sir.

Many web pages are copied and republished across sites - Our favorite copy-paste action, right. 
So now we need to perform the de-duplication process.
To do this we will run algorithms to remove not only repeated text fragments but even near-duplicate documents.
This will later on help our model to grasp general rules from that data, so it can generalize and solve new situations instead of just memorizing old answers.
We need to make sure that our model can learn underlying principles, and not just specific examples.

[Visual: Animation of two nearly identical documents. One is deleted, the other remains. Overlay: "De-duplication".]

So after we've done all this filtration, what is left?
The end goal is a massive, diverse, and high-quality collection of texts that will serve as
a foundation that is broad enough to teach a model about everything 
from Shakespeare to Stack Overflow, from medical research to sci-fi fan fiction.
It's not just a size that matters, but also quality and breadth.

[Recap Slide: Data pipeline – Raw crawl → Filtered text → Large, diverse, high-quality corpus]
[Visual: Flowchart animation. Step 1: Raw crawl (messy web). Step 2: Filtering (blocklists, language, deduplication). Step 3: Final dataset (books, code, articles, etc.)]

But why go to all this effort?
Well because a well-built language model learns not just words, but also context, common sense and even 
hidden connections like underlying relationships between words and phrases or ideas and concepts.

[Visual: Network diagram showing words and concepts connected by lines, some lines glowing to show "hidden connections".]

And all of this thanks to starting off from an excellent data.

[Cut to: Presenter on camera]

So in summary, developing a state-of-the-art language model begins with the painful, slow, and costly process of scraping, 
filtering, and anonymizing vast amounts of text on the scale of the entire internet.

And without this foundation, even the most advanced neural networks would be lost for words — quite literally.

[Outro Slide: Up Next – How is all this data represented and visualized?]
[Visual: Books and articles morphing into numbers, vectors, or colorful data points.]

Well, great, thank you for tuning in and in the next video, we will see what happens next, after we've got this gigantic digital library:
Like how do we actually represent all this text so that a computer Program, our model can start learning from it?
Soo see you later and don't go too far away..   