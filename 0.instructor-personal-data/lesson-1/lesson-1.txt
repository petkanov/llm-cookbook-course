[Intro Slide: Data Collection and Preparation – Where it All Begins]

Hey everyone, welcome to this section where we will start unpacking what's really going on inside the so-called term - "artificial intelligence",
and especially - large language models, that most of us by now, can not imagine working or live without.

[Visual: Quick montage of news headlines, social media posts, and video clips all mentioning "AI". Overlay: the letters "A.I." appear everywhere on screen.]

This term we hear and see on every single article, on every video, every news, like everything and  
everywhere nowadays is spammed with these two letters "A.I.". So much hype, it's just crazy.
There are even people for which this even looks like some sort of a mysterious intelligent entity that is living somewhere in a box! 
But once we drill down into the reality behind the "AI" label and strip away all the marketing hype 
- you will see there is absolutely nothing intelligent about it. 
These models in their original Base form are not reasoning, they are not having insights or understanding the world like we do. 
In its essence, under the hood - It's just a sophisticated statistical calculator that is spitting off the next most likely word to follow.
So we are definitely not in any kind of danger that these calculators will take over the world.
However, your job could be at risk. It's not that AI will replace humans, 
but rather that people who know how to use AI will easily replace those who don't. 
If you’re not actively using AI in your daily work, you may find yourself left behind.

[Visual: Animation of a black box labeled "AI". The box opens to reveal gears, numbers, and a calculator inside.]

[Manim: Show a probability distribution curve that morphs as words are generated. As one word appears, the curve shifts to show the changing probability distribution for the next possible tokens.]
But let's get back to our main story.
So what's really happening inside the LL4Ms - the main driving force powering the AI revolution? 
Where does this process actually start? 
How does a simple piece of text get transformed into what feels like an intelligent response? 
Let's explore where it all begins.

The first important distinction we need to make clear is that the model itself—like GPT-4o 
and the application that users interact with—like ChatGPT, which uses this model — are very different things.
ChatGPT, or any similar application, consists of many additional layers that do not exist within the model itself. 
These include features like memory, agents, tools, context management and many other components 
that make Applications like ChatGPT so useful, intelligent and fluent.

The LLM model itself is more of a like extremely sophisticated archive of the entire internet. 
It is completely passive bunch of data. It cannot read internet. It doesn't do any actions. 
Think of it as a Highly advanced statistical calculator that was merged with the ZIP Archive of the entire Internet.
Which in the end will not do anything on its own until you press the button.
So The model by itself is absolutely passive file of Archived data In form of mathematical matrices.

Okay, we got it, it's sort of a zip file, but what's inside it? 
How is it going to conquer the world, right? Or at least make that impression.
So Inside the Large language models, it's all about predicting the next word — well, actually, not even the whole word. 
Most of the time, it's just a fragment of a word, that is called a token. 
And each token corresponds to a number. And the model's job is to spit out one number at a time 
using statistical mathematics. It does this based on everything it has already predicted prior to that word.
So the main task of LLM is to predict the next token, based on all previously predicted tokens.

[Visual: Close-up animation of a sentence being typed, but each word appears as a sequence of colored blocks (tokens). Each block flashes as it is "predicted".]

[Manim: Visualization of tokenization: show a sentence being split into tokens, each token transforming into its numerical representation. Then show vectors in a high-dimensional space representing these tokens with arrows pointing to the next most probable token.]

[Slide: "AI" = statistical prediction, not human thinking]

There's absolutely no actual thinking happening — just an advanced pattern matching and probability calculations.
So by the end of these lessons, you'll see for yourself exactly how these systems work.
You will understand not only how this imitation of an intelligent output is generated, 
but also how you can use these tools in your own projects, so that you can build applications that are smart, 
extremely powerful and can make a real difference in the world.

[Visual: Split screen. Left side: a human brain with lightbulbs and creative ideas around it. Right side: a robot that looks like a mix between a calculator and a robot (robotic body with calculator buttons, display, and mechanical features), surrounded by numbers, graphs, and data visualizations. Add the text: "Pattern matching, not thinking". Make sure the human brain is only on the left, and the robot-calculator hybrid is only on the right, with no brain elements on the right side.]

[Manim: Animated Markov chain showing how probabilities flow through a sequence of tokens, with conditional probability formulas appearing and evolving alongside the visualization.]

Now let's start our AI exploration journey and together, we are going to debunk 
the myth of intelligence in this "AI" Marketing term — and, more importantly,
we will learn how to use these tools' true power for our own benefit.

[Natural pause. Shift to Section 1.]

[Visual: Map or journey graphic, with a path labeled "AI Exploration" and milestones for each lesson.]

So let's begin with the very first step: 
We need somehow to teach this computer program, or a model, to speak a new language — and not just perfectly, 
but as richly and creatively as possible. How how would you do that?
You wouldn't just throw them a single book, or make them listen to the same conversation over and over.
Instead, you'd expose them to all kinds of books, news articles, blogs, scripts, casual chats — 
pretty much every piece of text that you could get your hands on.
The more diverse, comprehensive, and high-quality the text content is, the better our program's eventual language skills will be.

[Visual: Person (cartoon or real) surrounded by a swirling cloud of books, newspapers, web pages, chat bubbles, and scripts.]

So now we got our first problem - how do we actually collect and prepare that massive amount of text that our model will learn from?

[Visual: Giant funnel graphic. At the top: icons for books, websites, forums, chats. At the bottom: a hard drive labeled "Training Data".]

[Manim: Visualization of exponential growth curve showing data scale, with animated counter displaying gigabytes growing to terabytes, alongside visualization of token count growing to billions.]

And we are talking about collecting data in the scale of tens of terabytes, that contains hundreds of billions of words.
The most famous and available example of something that we are looking for - is the FineWeb dataset, that's managed by Hugging Face.
It's exactly what we need - a filtered, high quality set of internet text that weighs in at around 44 terabytes.
which by today's measures it's actually really not that much.
But how did they do it and what is it and where does all this data even come from?

[Slide: FineWeb by Hugging Face – 44TB of filtered text]
[Visual: Screenshot of the FineWeb dataset page on Hugging Face. Overlay: "44TB" in bold.]

So most of it originates from what's called the Common Crawl.
Think of Common Crawl as the giant web archive of our time.
Since I think 2007, this Common Crawl has been systematically reading and archiving the public internet.
And by now, it has crawled nearly 3 billion web pages.
But this isn't just about website addresses — no, - it's their entire content: which is articles, documentation, forums, chats and more.

[Visual: Illustration of a robot spider crawling traversing the internet, collecting web pages into a giant digital library.]

[Manim: Graph theory visualization showing a network of interconnected nodes representing websites, with crawler paths highlighted as they traverse the graph, with counters tracking visited pages and data collected.]

But of course, not every piece of internet text is useful, or even worth feeding into our model.
Raw web data is really messy. Think of this - it includes spam, duplicated copy-paste pages, messed up low-quality content, 
and of course - texts in many, many different languages. 
So, now we need to clean and filter all this data carefully.

[Visual: Messy pile of documents, some with red Xs (spam, duplicates, foreign languages), others with green checks (good content).]

[Slide: Keyword – Filtering: Making Raw Data Usable]

And we arrived at our first step, which is - URL blocklists.

Think of it as a gitignore file in which you would write all the files or directories that you do not want to include in your commits.
But in Common Crawl case it's a file that includes website addresses, the content of which it will exclude from crawling and scraping.
And why do we want to do this? 
That's because all those websites from that list are known for malware, spam, adult content, or whatever else unwanted material.

[Visual: List of URLs on screen, some highlighted in red and crossed out. Overlay: "BLOCKED" stamp.]

[Manim: Set theory visualization showing Venn diagrams of total web content and filtered content, with mathematical notation for set operations (union, intersection, difference) as content is filtered.]

Ok, so we managed to avoid a few forbidden websites, we scraped the others and now we got all that websites data.
So what's next? 
Next we need to extract the actual text from HTML markup.
And as you know, the Web content is wrapped up in a lot of code — HTML, JavaScript, CSS.
What we need is to strip all that away, so that we get the text that people actually read.

[Screen demo: Split screen. Left: Raw HTML code. Right: Clean, readable text. Animation: HTML tags fade away, leaving just the text.]

Then comes language filtering.
Typically, we want the majority of our dataset to be in English, since that is where our model is expected to be the most fluent in.
A common threshold is to require that at least 65% of a website content should be in English in order for its content to be included.

[Slide: Language filtering – Pie chart: >65% English]
[Visual: Pie chart animation showing a slice labeled "English" growing to 65% or more.]

[Manim: Statistical sampling visualization showing how language detection algorithms work with confidence intervals and thresholds, with mathematical notation for statistical significance.]

Okay, now we got a text, we filtered away all the unnecessary code and data. 
And now we ended up with a nice and clean English text only. 
But we're still not finished. No Sir.

Many web pages are copied and republished across sites - Our favorite copy-paste action, right. 
So now we need to perform the de-duplication process.
To do this we will run algorithms to remove not only repeated text fragments but even near-duplicate documents.
This will later on help our model to grasp general rules from that data, so it can generalize and solve new situations instead of just memorizing old answers.
We need to make sure that our model can learn underlying principles, and not just specific examples.

[Visual: Illustration of two nearly identical documents. One is deleted, the other remains. Overlay: "De-duplication".]

[Manim: Algorithm visualization showing similarity metrics between documents using cosine similarity, with vectors in space moving closer together when documents are similar, and a threshold line showing when documents are considered duplicates.]

So after we've done all this filtration, what is left?
The end goal is a massive, diverse, and high-quality collection of texts that will serve as
a foundation that is broad enough to teach a model about everything 
from Shakespeare to Stack Overflow, from medical research to sci-fi fan fiction.
It's not just a size that matters, but also quality and breadth.

[Recap Slide: Data pipeline – Raw crawl → Filtered text → Large, diverse, high-quality corpus]
[Visual: Flowchart Illustration. Step 1: Raw crawl (messy web). Step 2: Filtering (blocklists, language, deduplication). Step 3: Final dataset (books, code, articles, etc.)]

But why go to all this effort?
Well because a well-built language model learns not just words, but also context, common sense and even 
hidden connections like underlying relationships between words and phrases or ideas and concepts.

[Visual: Network diagram showing words and concepts connected by lines, some lines glowing to show "hidden connections".]

[Manim: Word embedding visualization in 3D space, showing how semantically similar words cluster together, with mathematical transformation equations showing how words become vectors.]

And all of this thanks to starting off from an excellent data.

[Cut to: Presenter on camera]

So in summary, developing a state-of-the-art language model begins with the painful, slow, and costly process of scraping, 
filtering, and anonymizing vast amounts of text on the scale of the entire internet.

And without this foundation, even the most advanced neural networks would be lost for words — quite literally.

[Outro Slide: Up Next – How is all this data represented and visualized?]
[Visual: Books and articles morphing into numbers, vectors, or colorful data points.]

[Manim: Final mathematical animation showing the entire pipeline from raw text to token vectors to neural network processing, with mathematical notations for each transformation step.]

Well, great, thank you for tuning in and in the next video, we will see what happens next, after we've got this gigantic digital library:
Like how do we actually represent all this text so that a computer Program, our model can start learning from it?
Soo see you later and don't go too far away..   